{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb44895",
   "metadata": {},
   "source": [
    "# AMS516 Homework 1. Continuous-time stochastic linear quadratic regulators (LQR)\n",
    "\n",
    "## Juan Perez Osorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52dbb5",
   "metadata": {},
   "source": [
    "# **1. An infinite-horizon discounted stochastic LQR problem**\n",
    "\n",
    "Consider the following continuous-time dynamics:\n",
    "\n",
    "$$dx_t = (Ax_t + Bu_t)dt + Gdw_t$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x_t \\in \\mathbb{R}^n$ is the state vector\n",
    "* $u_t \\in \\mathbb{R}^m$ is the control input\n",
    "* $w_t$ is a standard $r$-dimensional Wiener process\n",
    "* $G \\in \\mathbb{R}^{n \\times r}$ is the noise gain matrix\n",
    "\n",
    "The discounted quadratic cost is given by:\n",
    "\n",
    "$$J(x_0; u) = \\mathbb{E}\\left[ \\int_0^\\infty e^{-\\rho t}(x_t^\\intercal Q x_t + u_t^\\intercal R u_t) dt \\right]$$\n",
    "\n",
    "with:\n",
    "\n",
    "* Discount rate $\\rho > 0$\n",
    "* $Q \\geq 0$ (positive semidefinite)\n",
    "* $R \\geq 0$ (positive semidefinite)\n",
    "\n",
    "The goal is to find optimal feedback control $u_t = -Kx_t$ minimizing $J$.\n",
    "\n",
    "The Hamilton-Jacobi-Bellman (HJB) equation for the discounted cost is:\n",
    "\n",
    "$$\\rho V(x) = \\min_u \\left\\{x^\\intercal Q x + u^\\intercal R u + \\nabla V(x)^\\intercal (Ax + Bu) + \\frac{1}{2} \\text{tr}(GG^\\intercal \\nabla^2 V) \\right\\}$$\n",
    "\n",
    "The value function $V(x)$ and the optimal control $u$ have the form:\n",
    "\n",
    "$$V(x) = x^\\intercal P x + c$$\n",
    "$$u = -R^{-1} B^\\intercal P x$$\n",
    "\n",
    "Moreover, $P$ satisfies the discounted continuous algebraic Riccati equation (CARE)\n",
    "\n",
    "$$A^\\intercal P + PA - P B R^{-1} B^\\intercal P + Q - \\rho P = 0$$\n",
    "\n",
    "and $c$ is given by:\n",
    "\n",
    "$$c = \\frac{\\operatorname{tr}(G G^T P)}{\\rho}.$$\n",
    "\n",
    "Consider the problem with the following parameters:\n",
    "\n",
    "$$\n",
    "\\rho = 0.1, \\quad A = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad G = \\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}, \\quad Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad R = [1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc22670",
   "metadata": {},
   "source": [
    "(a). Use the analytical result above to compute the value function $V(x)$ and optimal control $u$ (see the sample code in Appendix 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1402135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve_continuous_are, solve_lyapunov, eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d975b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho: float = 0.1\n",
    "\n",
    "A = np.array([[0, 1], [-2, -3]])\n",
    "B = np.array([[0], [1]])\n",
    "G = np.array([[0.1, 0], [0, 0.1]])\n",
    "Q = np.array([[1, 0], [0, 1]])\n",
    "R = np.array([[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc45da",
   "metadata": {},
   "source": [
    "We can see that when checking the library for the function ``solve_continuos_are``, we found that it solves the continuos-time algebraic Riccati equation (CARE):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A^H X + XA - X B R^{-1} B^H X + Q = 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But we now that $P = X$ satisfies the discounted continuous algebraic Riccati equation (CARE):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "A^\\intercal P + PA - P B R^{-1} B^\\intercal P + Q - \\rho P = 0\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "So we need to turn our last equation into the form of (1) so we can apply the scipy method. To do so we manipulate the expression in the following way\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "A^\\intercal P + PA - P B R^{-1} B^\\intercal P + Q - \\rho P &= 0 \\\\\n",
    "A^\\intercal P + PA - P B R^{-1} B^\\intercal P + Q  &= \\rho P\\\\\n",
    "A^\\intercal P + PA - P B R^{-1} B^\\intercal P + Q  &= \\frac{1}{2}\\rho (IP + PI)\\\\\n",
    "\\left(A^\\intercal P - \\frac{1}{2} \\rho IP \\right) + \\left(PA - \\frac{1}{2} \\rho PI \\right) - P B R^{-1} B^\\intercal P + Q  &= 0 \\\\\n",
    "\\left(A^\\intercal - \\frac{1}{2} \\rho I \\right)P + P\\left(A - \\frac{1}{2} \\rho I \\right) - PBR^{-1} B^\\intercal P + Q  &= 0 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Letting $A_{\\text{disc}} = A - \\frac{1}{2} \\rho I$, we can see that the previous equation reduces to \n",
    "\n",
    "$$\n",
    "A^{\\intercal}_{\\text{disc}}P + PA_{\\text{disc}} - PBR^{-1} B^\\intercal P + Q  = 0\n",
    "$$\n",
    "\n",
    "we can see that our final expression is now in the same form as (1) and hence, we can apply the scipy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6853bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(A.shape[0])\n",
    "\n",
    "# Modify A so that we can use the solve_continuos_are method from scipy\n",
    "A_discounted = A - (1/2) * rho * I\n",
    "\n",
    "P = solve_continuous_are(a = A_discounted, b = B, q = Q, r = R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f4655",
   "metadata": {},
   "source": [
    "The optimal gain matrix $K$ is defined to be\n",
    "\n",
    "$$K = R^{-1} B^\\intercal P$$\n",
    "\n",
    "so we have that \n",
    "\n",
    "$$u = -Kx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10699ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal gain matrix\n",
    "K = np.linalg.inv(R) @ B.T @ P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206e0b7",
   "metadata": {},
   "source": [
    "Replacing this in the time dynamics we have \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "dx_t &= (Ax_t + Bu_t)dt + Gdw_t \\\\\n",
    "     &= (Ax_t - BK x_t)dt + Gdw_t \\\\\n",
    "     &= (A- BK)x_tdt + Gdw_t \\\\\n",
    "     &= (A_{\\text{closed}})x_tdt + Gdw_t\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e560dad",
   "metadata": {},
   "source": [
    "Where we define $A_{\\text{closed}}$ to be the closed loop matrix and it describes the dynamics of a system with feedback control law, i.e., the closed-loop system is obtained by substituting the optimal control $u$ into the dynamics of the system.\n",
    "\n",
    "A nice way to checking the stability of the system is by checking the eigenvalues of the closed loop matrix\n",
    "\n",
    "> we can summarize how the two-dimensional linear dynamical system's stability depends on  $\\operatorname{Tr}(A)$ and  $det(A)$ in a simple diagram as shown\n",
    "\n",
    "![stability](./images/stability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad621d4",
   "metadata": {},
   "source": [
    "Hence we have to check that all the eigenvalues of the closed loop matrix are negative so that our system is stable; we do this procedure to confirm that the solution $P$ from the (discounted) CARE produces a stabilizing controller $K$. If any eigenvalue had a positive real part, the controller would be useless as the state would blow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc0f7760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigenvalues of A_cl:\n",
      "[-0.98838969+0.j -2.23620796+0.j]\n",
      "Is the closed-loop system stable? True\n"
     ]
    }
   ],
   "source": [
    "# Stability analysis\n",
    "A_cl = A - B @ K\n",
    "eig_A_cl = eigvals(A_cl)\n",
    "\n",
    "print(\"\\nEigenvalues of A_cl:\")\n",
    "print(eig_A_cl)\n",
    "\n",
    "# Check if all real parts are negative (system is stable)\n",
    "is_stable = np.all(np.real(eig_A_cl) < 0)\n",
    "print(f\"Is the closed-loop system stable? {is_stable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff4736",
   "metadata": {},
   "source": [
    "Let’s define $X_t = \\mathbb{E}[x_t x_t^T]$. This is the covariance matrix of the state at time $t$. Its evolution is governed by the SDE. Itô's Lemma tells us how to find the differential $d(x_t x_t^T)$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "d(x_t x_t^\\intercal) =& \\left[d(x_t)\\right]x^{\\intercal}_{t} + x_t \\left[d(x^{\\intercal}_{t})\\right] + \\left[d(x_t)\\right]\\left[d(x^{\\intercal}_{t})\\right] \\\\\n",
    "\n",
    "=& \\left[A_{\\text{closed}}x_tdt + Gdw_t\\right]x^{\\intercal}_{t} + x_t \\left[(A_{\\text{closed}}x_tdt + Gdw_t)^\\intercal\\right] + \\left[d(x_t)\\right]\\left[d(x^{\\intercal}_{t})\\right] \\\\\n",
    "\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "but we know that \n",
    "\n",
    "$$[d(x_t)][d(x_t)]^\\intercal = (G dw_t)(G dw_t)^\\intercal = G (dw_t dw_t^\\intercal) G^\\intercal = G (I dt) G^\\intercal = G G^\\intercal dt$$\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "$$ d(x_t x_t^\\intercal) = \\left(A_{\\text{closed}}x_tdt + Gdw_t\\right)x^{\\intercal}_{t} + x_t \\left(A_{\\text{closed}}x_tdt + Gdw_t\\right)^\\intercal + G G^\\intercal dt$$\n",
    "\n",
    "We want an equation for $dX_t = d(\\mathbb{E}[x_t x_t^\\intercal]) = \\mathbb{E}[d(x_t x_t^\\intercal)]$.\n",
    "\n",
    "* $\\mathbb{E}[G dw_t x_t^\\intercal] = 0$ and $\\mathbb{E}[x_t dw_t^\\intercal G^\\intercal] = 0$ because the current state $x_t$ is uncorrelated with the future noise $dw_t$.\n",
    "\n",
    "* $\\mathbb{E}[A_{cl} x_t x_t^\\intercal dt] = A_{cl} \\mathbb{E}[x_t x_t^\\intercal] dt = A_{cl} X_t dt$\n",
    "\n",
    "* $\\mathbb{E}[x_t x_t^\\intercal A_{cl}^\\intercal dt] = X_t A_{cl}^\\intercal dt$\n",
    "\n",
    "* $\\mathbb{E}[G G^\\intercal dt] = G G^\\intercal dt$\n",
    "\n",
    "Hence we have that\n",
    "\n",
    "$$ dX_t = \\mathbb{E}[d(x_t x_t^\\intercal)] = A_{\\text{closed}}X_tdt + X_{t}A_{\\text{closed}}^{\\intercal}dt + GG^\\intercal dt$$\n",
    "\n",
    "$$ \\frac{dX_t}{dt} = A_{\\text{closed}}X_t + X_{t}A_{\\text{closed}}^{\\intercal} + GG^\\intercal$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc86ff9",
   "metadata": {},
   "source": [
    "At steady-state, the system's statistical properties don't change anymore. This means the covariance matrix $X_t$ settles to a constant matrix $\\Sigma$, so its derivative becomes zero $dX_t / dt = 0$. Hence, at steady state we have that\n",
    "\n",
    "$$\n",
    "A_{\\text{closed}}\\Sigma + \\Sigma A_{\\text{closed}}^{\\intercal} + GG^\\intercal = 0\n",
    "$$ \n",
    "\n",
    "This is the famous **Lyapunov equation**. To solve it we use the ``solve_lyapunov`` from the scipy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c599c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho =  0.1\n",
      "P = \n",
      " [[1.14817499 0.2102449 ]\n",
      " [0.2102449  0.22459765]]\n",
      "K = \n",
      " [[0.2102449  0.22459765]]\n",
      "Sigma = \n",
      " [[ 0.00954679 -0.005     ]\n",
      " [-0.005       0.00497774]]\n",
      "Value constant c =  0.13727726484530767\n",
      "J(x0) =  1.2854522598310418\n"
     ]
    }
   ],
   "source": [
    "# Steady state covariance\n",
    "GGt = G @ G.T\n",
    "\n",
    "# Solves the continuous Lyapunov equation AX + XA^H = Q\n",
    "S = solve_lyapunov(A_cl, -GGt)\n",
    "\n",
    "# The discounted value constants are given by\n",
    "c = np.trace(GGt @ P) / rho\n",
    "\n",
    "# Example: initial state and optimal discounted cost\n",
    "x0 = np.array([1.0, 0])\n",
    "J0 = x0.T @ P @ x0 + c\n",
    "\n",
    "print('rho = ', rho)\n",
    "print('P = \\n', P)\n",
    "print('K = \\n', K)\n",
    "print('Sigma = \\n', S)\n",
    "print('Value constant c = ', c)\n",
    "print('J(x0) = ', J0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178c5ac",
   "metadata": {},
   "source": [
    "(b) Show that the optimal control $u$ in (3) is given by $u^∗ = −R^{−1}B^\\intercal \\nabla V$ . Plug $u^∗$ into (3), we obtain a differential equation of $V$. Use the PINN method to solve the obtained\n",
    "equation for $V$ and $u$. Denote your solution by $\\hat{V}$ and $\\hat{u}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc05dc9",
   "metadata": {},
   "source": [
    "We know that the HJB equation for the discounted cost is \n",
    "\n",
    "$$\\rho V(x) = \\min_u \\left\\{x^\\intercal Q x + u^\\intercal R u + \\nabla V(x)^\\intercal (Ax + Bu) + \\frac{1}{2} \\text{tr}(GG^\\intercal \\nabla^2 V) \\right\\}$$\n",
    "\n",
    "as we can see, not all terms inside the minimization objective contain $u$, hence, we can split this in two parts\n",
    "\n",
    "$$\\rho V(x) = \\min_u \\left\\{u^\\intercal R u + \\nabla V(x)^\\intercal Bu \\right\\} + \\left[x^\\intercal Q x + \\nabla V(x)^\\intercal Ax + \\frac{1}{2} \\text{tr}(GG^\\intercal \\nabla^2 V) \\right]$$\n",
    "\n",
    "Since we want to find the $u$ that minimizes the expression, we can focus solely on the part that depends on $u$, let's define this part by $f$\n",
    "\n",
    "$$ f(u) = u^\\intercal R u + \\nabla V(x)^\\intercal Bu $$\n",
    "\n",
    "To find the minimum of the function $g(u)$ with respect to the vector $u$, we take its gradient (derivative with respect to $u$) and set it equal to zero. Computing the Gradient $\\nabla_u f(u)$ we obtain that \n",
    "\n",
    "$$\\nabla_u f(u) = (R + R^\\intercal)u + B^\\intercal \\nabla V(x)$$\n",
    "\n",
    "But since $R$ is symmetric we have that\n",
    "\n",
    "$$\\nabla_u f(u) = 2Ru + B^\\intercal \\nabla V(x) = 0$$\n",
    "$$2Ru = -B^\\intercal \\nabla V(x)$$\n",
    "$$u^* = -\\frac{1}{2}R^{-1}B^\\intercal \\nabla V(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8f4e5",
   "metadata": {},
   "source": [
    "Plugging this into the HJB equation for the discounted cost we can notice that\n",
    "\n",
    "$$\\nabla V(x)^\\intercal B u^* = \\nabla V(x)^T B (-R^{-1}B^T \\nabla V) = -\\frac{1}{2}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "{u^*}^\\intercal R u^* &= \\left(-\\frac{1}{2}R^{-1}B^\\intercal \\nabla V\\right)^\\intercal R \\left(-\\frac{1}{2}R^{-1}B^\\intercal \\nabla V\\right) \\\\\n",
    "                      &= \\frac{1}{4}\\nabla V^\\intercal B (R^{-1})^\\intercal RR^{-1}B^\\intercal \\nabla V \\\\\n",
    "                      &= \\frac{1}{4}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V \\\\\n",
    "\\end{split}\n",
    "$$ \n",
    "\n",
    "Now we plug these back in. Notice that \n",
    "\n",
    "$$(u^*)^\\intercal R u^* + \\nabla V(x)^\\intercal B u^* = \\frac{1}{4}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V - \\frac{1}{2}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V = - \\frac{1}{4}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V$$\n",
    "\n",
    "Therefore we have that the HJB equation for the discounted cost takes the form\n",
    "\n",
    "$$\\rho V(x) = x^\\intercal Q x + \\nabla V(x)^\\intercal Ax - \\frac{1}{4}\\nabla V^\\intercal B R^{-1} B^\\intercal \\nabla V + \\frac{1}{2} \\text{tr}(GG^\\intercal \\nabla^2 V)$$\n",
    "\n",
    "So, we can define our loss function to be\n",
    "\n",
    "$$L_{\\operatorname{physics}} = \\left| \\rho V(x) - x^\\intercal Q x - \\nabla V(x)^\\intercal Ax + \\frac{1}{4}\\nabla V(x)^\\intercal B R^{-1} B^\\intercal \\nabla V(x) - \\frac{1}{2} \\text{tr}(GG^\\intercal \\nabla^2 V(x))\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be642d3",
   "metadata": {},
   "source": [
    "Now we're going to set up the PINN in torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85423f5",
   "metadata": {},
   "source": [
    "1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90908da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adab120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor created directly on cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device: allows us to specify whether computations should occur on the CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Test to see where the tensor was created\n",
    "tensor_on_device = torch.rand(5, device=device)\n",
    "print(f\"Tensor created directly on {tensor_on_device.device}\")\n",
    "\n",
    "# Convert the system parameters to torch tensors\n",
    "A = torch.tensor([[0.0, 1.0], [-2.0, -3.0]], device = device)\n",
    "B = torch.tensor([[0.0], [1.0]], device = device)\n",
    "G = torch.tensor([[0.1, 0.0], [0.0, 0.1]], device = device)\n",
    "Q = torch.tensor([[1.0, 0.0], [0.0, 1.0]], device = device)\n",
    "R = torch.tensor([[1.0]], device = device)\n",
    "R_inv = torch.linalg.inv(R)\n",
    "\n",
    "# Pre-compute useful matrices\n",
    "B_Rinv_BT = B @ R_inv @ B.T\n",
    "GGt = G @ G.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dce46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation = 'tanh', input_shape = (1,)),\n",
    "    tf.keras.layers.Dense(10, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Define the physics loss and boundary loss\n",
    "def loss(model, x, f):\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        tape.watch(x)\n",
    "        u = model(x)\n",
    "        u_x = tape.gradient(u, x)\n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "    \n",
    "    physics_loss = tf.reduce_mean(tf.square(u_xx + f))\n",
    "\n",
    "    # Boundary loss\n",
    "    u_0 = model(tf.convert_to_tensor([[0.0]], dtype = tf.float32))\n",
    "    u_1 = model(tf.convert_to_tensor([[1.0]], dtype = tf.float32))\n",
    "    boundary_loss = tf.square(u_0) + tf.square(u_1)\n",
    "\n",
    "    return physics_loss + boundary_loss\n",
    "\n",
    "# Training data\n",
    "x = tf.convert_to_tensor(np.linspace(0, 1, 100).reshape(-1, 1), dtype = tf.float32)\n",
    "\n",
    "f = np.pi ** 2 * np.sin(np.pi * x)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model, x, f)\n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss = {current_loss.numpy()}')\n",
    "\n",
    "# Evaluate the trained model\n",
    "u_pinn = model.predict(x)\n",
    "u_exact = np.sin(np.pi * x)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, u_pinn, label = \"PINN Solution\")\n",
    "plt.plot(x, u_exact, label = \"Exact Solution\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, u_pinn - u_exact, '--', label = \"Error\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
